{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this Python notebook, we are doing topic modeling based on available Python packages in this research field.\n",
    "\n",
    "**IMPORTANT**: this notebook requires the `java` module. If it was not loaded in JupyterLab, you must close this notebook, stop the kernel, load the `java` module and re-open the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard and scientific packages\n",
    "print('- Loading standard modules...')\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# NLTK - Natural Language Toolkit\n",
    "print('- Loading NLTK...')\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Only required on the first execution\n",
    "\n",
    "# Gensim\n",
    "print('- Loading Gensim...')\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "\n",
    "# spaCy for lemmatization\n",
    "print('- Loading spaCy...')\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "print('- Loading visualization tools...')\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "print('- End of configuration...')\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "* Load stop words from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# See the default list\n",
    "print('Default list:\\n', stop_words)\n",
    "\n",
    "# Add your custom stop words\n",
    "stop_words.extend([])\n",
    "\n",
    "# See the final list of stop words\n",
    "print('\\nFinal list:\\n', stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the list of filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path and extension of text files\n",
    "txt_folder = Path('data/').rglob('*.txt')\n",
    "\n",
    "files = sorted([x for x in txt_folder])  # Gather text files paths in a list\n",
    "print(files[:3], '...', files[-3:])  # Print first 3 and last 3 filenames\n",
    "print(f' => total {len(files)} files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a dictionary that will populate a Pandas DataFrame with two columns:\n",
    "  * `target_names`: the filename without its path\n",
    "  * `content`: the original text data of the file in single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {'target_names': [], 'content': []}\n",
    "\n",
    "# For each text file\n",
    "for name in files:\n",
    "    f = open(name, 'r', encoding='utf-8')\n",
    "    basename = os.path.basename(name)\n",
    "\n",
    "    # Print the progression at every 10 filenames\n",
    "    if name in files[::10]:\n",
    "        print(f'Reading {basename} ...')\n",
    "\n",
    "    # Save the filename and the file content\n",
    "    text_dict['target_names'].append(basename)\n",
    "    text_dict['content'].append(' '.join(f.readlines()))\n",
    "    f.close()\n",
    "\n",
    "# Convert the dictionary to a pandas data frame \n",
    "df = pd.DataFrame.from_dict(text_dict)\n",
    "print(f'Total: {len(df)} rows. Here are the first five:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text data\n",
    "* Remove roman numerals and multiple spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all file contents\n",
    "data = text_dict['content']\n",
    "\n",
    "# Remove roman numerals\n",
    "data = [re.sub('[MDCLXVI]+(\\.|\\b\\w\\n)', ' ', sentence) for sentence in data]\n",
    "\n",
    "# Replace new line characters and multiple spaces by a single space\n",
    "data = [re.sub('\\s+', ' ', sentence) for sentence in data]\n",
    "\n",
    "# Remove distracting quotes\n",
    "#data = [re.sub(\"\\'\", \"\", sentence) for sentence in data]\n",
    "\n",
    "print('First cleaned sentence:\\n', data[0])\n",
    "print('\\nLast cleaned sentence:\\n', data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove punctuation symbols and transform each text into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_words(sentences):\n",
    "    \"\"\"\n",
    "    Generator - For each sentence, return a processed list of words\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    Each sentence processed by gensim.utils.simple_preprocess(), which\n",
    "    removes the punctuation and collects all the individual words.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        # Setting deacc=True removes punctuations\n",
    "        yield(simple_preprocess(sentence, deacc=True))\n",
    "\n",
    "# Create a list of lists of words - one list of words per sentence\n",
    "data_words = list(sentences_to_words(data))\n",
    "\n",
    "print('First list of words:', data_words[0])\n",
    "print('\\nLast list of words:', data_words[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "We will start by using:\n",
    "* Gensim's [Phrases class](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases) - an instance of it \"detects phrases based on collocation counts\"\n",
    "* Gensim's [Phraser class](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phraser) - an alias of [FrozenPhrases](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.FrozenPhrases) which \"cuts down memory consumption of Phrases, by discarding model state not strictly needed for the phrase detection task\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models - higher threshold => fewer phrases\n",
    "bigram = gensim.models.phrases.Phrases(data_words, min_count=4, threshold=8)\n",
    "trigram = gensim.models.phrases.Phrases(bigram[data_words], threshold=8)\n",
    "\n",
    "# Faster way to get a sentence identified as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See a trigram example\n",
    "for word in trigram_mod[bigram_mod[data_words[90]]]:\n",
    "    if len(word.split('_')) == 3:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define functions for stopwords, bigrams, trigrams and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Complete the cleanup of word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('- Removing Stop Words...')\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "print('- Forming bigrams...')\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "print('- Forming trigrams...')\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "print('- Initializing the spaCy model...')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "print('- Lemmatisation...')\n",
    "data_lemmatized = lemmatization(data_words_trigrams,\n",
    "                                allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the dictionnary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "\n",
    "# Readable format of corpus\n",
    "[[(id2word[id], freq) for id, freq in cp[:10]] for cp in corpus[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2   # Set the minium number of topics your model will run\n",
    "limit = 10  # Choose the max ceiling for number of topics\n",
    "step = 2    # Set the step width for number of topics per model\n",
    "multiple_num_topics = range(start, limit + 1, step)\n",
    "\n",
    "model_list = []\n",
    "coherence_values = []\n",
    "\n",
    "for num_topics in multiple_num_topics:\n",
    "    print(f'With {num_topics} topics...')\n",
    "\n",
    "    model = LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        num_topics=num_topics,\n",
    "        id2word=id2word,\n",
    "        workers=1)\n",
    "    model_list.append(model)\n",
    "\n",
    "    coherencemodel = CoherenceModel(\n",
    "        model=model,\n",
    "        texts=data_lemmatized,\n",
    "        dictionary=id2word,\n",
    "        coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "plt.plot(multiple_num_topics, coherence_values)\n",
    "\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(multiple_num_topics, coherence_values):\n",
    "    print(f'Num Topics = {m:2d},',\n",
    "          f'has Coherence Value of {round(cv, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which model in the list you think is the best\n",
    "# Remember python started indexing from 0\n",
    "optimal_model = model_list[3]\n",
    "\n",
    "# Showing different topics\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run just that model with the exact number of topics you want\n",
    "ldamallet = LdaMulticore(corpus=corpus, num_topics=8, id2word=id2word, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# See the Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(\n",
    "    model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=df):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                    pd.Series(\n",
    "                        [int(topic_num), round(prop_topic,4), topic_keywords]),\n",
    "                    ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    sent_topics_df.columns = [\n",
    "        'Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = texts\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final results\n",
    "df_topic_sents_keywords = format_topics_sentences(\n",
    "    ldamodel=ldamallet, corpus=corpus, texts=df)\n",
    "\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "    'Document number',\n",
    "    'Dominant_Topic',\n",
    "    'Topic_Perc_Contrib',\n",
    "    'Keywords',\n",
    "    'file_name',\n",
    "    'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 Kernel",
   "language": "python",
   "name": "20220211_1755"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
