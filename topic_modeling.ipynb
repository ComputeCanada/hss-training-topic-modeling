{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy gensim pyLDAvis spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General stuff | trucs généraux\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "\n",
    "# spacy for lemmatization | spacy pour lemmatisation\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Plotting tools | outils graphiques\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional | activé le registre pour gensim - en option\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words | NLTK Mots vides\n",
    "from nltk.corpus import stopwords\n",
    "#stop_words = stopwords.words('english')\n",
    "stop_words = stopwords.words('french')\n",
    "print(stop_words) #see the default list | voir la liste pas défaut\n",
    "stop_words.extend([])  #add your custom stop words | ajoutez vos mots vides personnalisés \n",
    "print(stop_words) #see the final list of stop words | voir la liste complète"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt_folder = Path('data_folder/').rglob('*.txt') #gather the paths for all your text files \n",
    "txt_folder = Path('donnee/').rglob('*.txt') #collecter les chemins de fichiers pour tous vos fichiers texte\n",
    "files = [x for x in txt_folder] \n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary which contains all the file names and matches them to their text contents\n",
    "#créer un dictionnaire qui contient tous les noms de fichiers et les associe à leur texte\n",
    "papers={}\n",
    "papers['target_names']=[]\n",
    "papers['content']=[]\n",
    "for name in files:\n",
    "    f = open(name, 'r', encoding='utf-8')\n",
    "    print(str(name).split('\\\\')[1])\n",
    "    papers['target_names'].append(str(name).split('\\\\')[1])\n",
    "    papers['content'].append(' '.join(f.readlines()))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dictionary to a pandas data frame \n",
    "#convertir le dictionnaire en dataframe pandas\n",
    "df = pd.DataFrame.from_dict(papers)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text content to a list\n",
    "# Convertir le contenu du texte en liste\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "#Remove roman numerals | Supprimer les chiffres romains\n",
    "data = [re.sub('[MDCLXVI]+(\\.|\\b\\w\\n)', ' ', sent) for sent in data]\n",
    "\n",
    "#Remove new line characters | Supprimer les caractères de nouvelle ligne\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "\n",
    "#Remove distracting quotes | Supprimer les citations distrayantes\n",
    "#data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the punctuation and collect all the individual words\n",
    "#supprimer la ponctuation et collecter tous les mots individuels\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "#Construire les modèles bigramme et trigramme\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=10)  \n",
    "\n",
    "# Faster way to get a sentence identified as a trigram/bigram\n",
    "## Moyen plus rapide d'obtenir une phrase identifiée comme un trigramme/bigramme\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example | voir l'exemple trigramme\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "## Définir des fonctions pour les mots vides, les bigrammes, les trigrammes et la lemmatisation\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words | Supprimer les mots vides\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams | faire les bigrammes\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Form trigrams | faire les trigrammes\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "#nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# Initialiser le modèle spacy 'fr', en ne gardant que le composant tagger (pour plus d'efficacité)\n",
    "#nlp = spacy.load('fr_core_news_sm', disable=['parser', 'ner'])\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "# Faire la lemmatisation en ne gardant que le nom, l'adj, le vb, l'adv\n",
    "data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary | créer le dictionnaire\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus | créer le corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency | Durée Document Fréquence\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readable format of corpus | format lisible du corpus\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Ceiling for num of topics, model will evaluate up to but not including this number\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path=mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run. \n",
    "# il peut executé très longtemps\n",
    "set_limit = 11  #choose the max ceiling for number of topics, your model will have a max of one less than this ceiling \n",
    "#choisissez le nombre maximum plafond de thème, votre modèle aura un thème de moins \n",
    "set_start = 2 #set the minium number of topics your model will run | choisissez le nombre minimum de thème\n",
    "set_step = 2 #set the step width for number of topics per model | choisissez la taille du pas\n",
    "mallet_path = 'C:/Users/lydia/mallet-2.0.8/bin/mallet' # update this path to the path to your mallet program\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=set_start, limit=set_limit, step=set_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph | voir le graphique\n",
    "\n",
    "x = range(set_start, set_limit, set_step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores | voir les cohérences\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics|Numero de Théme =\", m, \" has Coherence Value of|a une cohérence de\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "#choissisez le mieux modéle et voir les thémes\n",
    "optimal_model = model_list[3] #choose which model in the list you think is the best, remember python started indexing from 0\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=8, id2word=id2word)  #now run just that model with the exact number of topics you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics | voir les thèmes\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# see the Coherence Score | voir la cohérance\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "#visualiser les thèmes\n",
    "# Can't use the gensim method for MALLET directly so converting LdaMallet Model to LdaModel as per https://radimrehurek.com/gensim/models/wrappers/ldamallet.html\n",
    "# Note that a \"by hand\" version of doing thing can be found at https://jeriwieringa.com/2018/07/17/pyLDAviz-and-Mallet/\n",
    "\n",
    "lda_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet, gamma_threshold=0.01, iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=df):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = texts\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=df)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document number','Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'file_name','Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
